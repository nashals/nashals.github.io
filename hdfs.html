<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>HDFS</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="HDFS"/>
<meta name="generator" content="Org-mode"/>
<meta name="author" content="dirtysalt"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style><link rel="shortcut icon" href="css/favicon.ico" /> <link rel="stylesheet" type="text/css" href="css/site.css" />


</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">HDFS</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="hdfs.html#sec-1">1 Opinions</a>
<ul>
<li><a href="hdfs.html#sec-1-1">1.1 Under the Hood: Hadoop Distributed Filesystem reliability with Namenode and Avatarnode</a></li>
<li><a href="hdfs.html#sec-1-2">1.2 HA Namenode for HDFS with Hadoop 1.0</a></li>
<li><a href="hdfs.html#sec-1-3">1.3 Why not RAID-0? It’s about Time and Snowflakes</a></li>
<li><a href="hdfs.html#sec-1-4">1.4 Hadoop I/O: Sequence, Map, Set, Array, BloomMap Files</a></li>
<li><a href="hdfs.html#sec-1-5">1.5 The Truth About MapReduce Performance on SSDs</a></li>
<li><a href="hdfs.html#sec-1-6">1.6 HDFS: Hadoop and Solid State Drive</a></li>
</ul>
</li>
<li><a href="hdfs.html#sec-2">2 Notes</a>
<ul>
<li><a href="hdfs.html#sec-2-1">2.1 HDFS Shell</a></li>
<li><a href="hdfs.html#sec-2-2">2.2 Filesystem Corruption and Missing Blocks</a></li>
<li><a href="hdfs.html#sec-2-3">2.3 文件系统API</a></li>
<li><a href="hdfs.html#sec-2-4">2.4 一致性问题</a></li>
<li><a href="hdfs.html#sec-2-5">2.5 读写进度</a></li>
<li><a href="hdfs.html#sec-2-6">2.6 获取集群运行状况</a></li>
<li><a href="hdfs.html#sec-2-7">2.7 All datanodes are bad. Aborting</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-3">
<h3 id="sec-1"><span class="section-number-3">1</span> Opinions</h3>
<div class="outline-text-3" id="text-1">


</div>

<div id="outline-container-1-1" class="outline-4">
<h4 id="sec-1-1"><span class="section-number-4">1.1</span> Under the Hood: Hadoop Distributed Filesystem reliability with Namenode and Avatarnode</h4>
<div class="outline-text-4" id="text-1-1">

<p><a href="http://www.facebook.com/notes/facebook-engineering/under-the-hood-hadoop-distributed-filesystem-reliability-with-namenode-and-avata/10150888759153920">http://www.facebook.com/notes/facebook-engineering/under-the-hood-hadoop-distributed-filesystem-reliability-with-namenode-and-avata/10150888759153920</a>
</p>
<p>
fb数据仓库故障有41%是源于HDFS，而如果有reliable namenode解决方案的话那么其中有90%是可以避免的。
</p>
<p>
<img src="images/hdfs-ha-namenode-avatarnode.png"  alt="./images/hdfs-ha-namenode-avatarnode.png" />
</p>
<p>
如果primary NN挂掉的话那么就切换到standby NN. datanode会将自己的status report到两个NN这样standby NN可以得到最新的状态可以使得切换时间更短。切换是通过zk来完成的，两个NN都在zk上面注册节点，client会从zk上了解primary NN对primary NN进行操作。之间的数据同步是通过共享存储来完成的，比如NFS，对于standby NN只需要增量读取操作内容即可。 #note: 大家对担心NFS的稳定性问题，不过我是觉得NFS上面主要是一些namenode上面一些操作的log，吞吐量不会太大而且也不会打开非常多的文件，在这个场景下面还是比较合适的
</p>
<p>
<img src="images/hdfs-avatarnode-view.png"  alt="./images/hdfs-avatarnode-view.png" />
</p>
</div>

</div>

<div id="outline-container-1-2" class="outline-4">
<h4 id="sec-1-2"><span class="section-number-4">1.2</span> HA Namenode for HDFS with Hadoop 1.0</h4>
<div class="outline-text-4" id="text-1-2">

<p><a href="http://hortonworks.com/blog/ha-namenode-for-hdfs-with-hadoop-1-0-part-1/">http://hortonworks.com/blog/ha-namenode-for-hdfs-with-hadoop-1-0-part-1/</a>
</p>
<p>
<b>Hadoop1 NameNode HA code failover with LinuxHA</b>
</p>
<p>
<img src="images/hdfs-ha-namenode-cold-failover.png"  alt="./images/hdfs-ha-namenode-cold-failover.png" />
</p>
<ul>
<li>Failover Times and Cold versus Hot Failover
<ul>
<li>The failover time of a high available system with active-passive failover is the sum of (1) time to detect that the active service has failed, (2) time to elect a leader and/or for the leader to make a failover decision and communicate to the other party, and (3) the time to transition the standby service to active.
</li>
<li>The first and second items are the same for cold or hot failover: they both rely on heartbeat timeouts, monitoring probe timeouts, etc. We have observed that total combined time for failure detection and leader election to range from 30 seconds to 2.5 minutes depending on the kind of failure; the lowest times are typical when the active server’s host or host operating system fails; hung processes take longer due to the grace period needed to be confident that the process is not blocked during Garbage Collection.
</li>
<li>For the third item, the time to transition the standby service to active, Hadoop 1 requires starting a second NameNode and for the NameNode to get out of safe mode. In our experiments we have observed the following times:
<ul>
<li>A 60 node cluster with 6 million blocks using 300TB raw storage, and 100K files: 30 seconds. Hence total failover time ranges from 1-3 minutes.
</li>
<li>A 200 node cluster with 20 million blocks occupying 1PB raw storage and 1 million files: 110 seconds. Hence total failover time ranges from 2.5 to 4.5 minutes.
</li>
</ul>

</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-3" class="outline-4">
<h4 id="sec-1-3"><span class="section-number-4">1.3</span> Why not RAID-0? It’s about Time and Snowflakes</h4>
<div class="outline-text-4" id="text-1-3">

<p><a href="http://hortonworks.com/blog/why-not-raid-0-its-about-time-and-snowflakes/">http://hortonworks.com/blog/why-not-raid-0-its-about-time-and-snowflakes/</a>
</p>
<ul>
<li>Reliability
<ul>
<li>Before panicking – disk failures are rare. Google’s 2007 paper, Failure Trends in a Large Disk Drive Population, reported that in their datacenters, 1.7% of disks failed in the first year of their life, while three-year-old disks were failing at a rate of 8.6%. About 9% isn’t a good number.（超过三年的硬盘发生问题的概率在9%） 8块超过3年的磁盘同时使用出现问题的概率在1-（1-0.086）^8 = 0.513，这个几率还是相当高的。这个还不是主要的问题，因为JBOD: Just a Box of Disks也会遇到这个问题。
</li>
<li>主要问题是，如果一旦一块磁盘出现问题的话，那么所有的磁盘上的数据都需要进行replication.因为RAID0是strip存储的，每个disk上面可能存储一个small block（64KB），而HDFS使用64MB作为block。这就意味着1个HDFS block在10 RAID0 disks上面的话会分摊在10个disk上面，如果一个disk出现问题的话，那么所有的HDFS block都发生损坏就都要进行replication
</li>
</ul>

</li>
<li>Every Disk is a Unique Snowflake
<ul>
<li>On RAID-0 Storage the disk accesses go at the rate of the slowest disk. RAID0带宽瓶颈限制在slowest disk上面
</li>
<li>The 2011 paper, <a href="http://static.usenix.org/event/hotos11/tech/final_files/Krevat.pdf">Disks Are Like Snowflakes: No Two Are Alike</a>, measured the performance of modern disk drives, and discovered that they can vary in data IO rates by 20%, even when they are all writing to same part of the hard disk.
</li>
<li>if you have eight disks, some will be faster than the others, right from day one. And your RAID-0 storage will deliver the performance of the slowest disk right from the day you unpack it from its box and switch it on.
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-4" class="outline-4">
<h4 id="sec-1-4"><span class="section-number-4">1.4</span> Hadoop I/O: Sequence, Map, Set, Array, BloomMap Files</h4>
<div class="outline-text-4" id="text-1-4">

<p><a href="http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/">http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/</a>
</p>
<p>
SequenceFile存储格式如下
<img src="images/hdfs-sequence-file-format.png"  alt="./images/hdfs-sequence-file-format.png" />
</p>
<p>
内部有三种可选的存储格式：
</p><ol>
<li>“Uncompressed” format
</li>
<li>“Record Compressed” format
</li>
<li>“Block-Compressed” format
</li>
</ol>


<p>
然后使用哪种格式以及元信息是在Header里面标记的
<img src="images/hdfs-sequence-file-header.png"  alt="./images/hdfs-sequence-file-header.png" />
</p>
<p>
其中metadata部分可以存储这个文件的一些元信息，存储格式也非常简单。key和value只是允许Text格式，并且在创建的时候就需要指定
<img src="images/hdfs-sequence-file-metadata.png"  alt="./images/hdfs-sequence-file-metadata.png" />
</p>
<p>
至于里面的record/block存储格式如下
<img src="images/hdfs-sequence-file-record.png"  alt="./images/hdfs-sequence-file-record.png" /> <img src="images/hdfs-sequence-file-block.png"  alt="./images/hdfs-sequence-file-block.png" />
至于Compress算法，这个在Header里面的Compress Codec Class Name里面就指定了。
</p>

<hr/>

<p>
Hadoop SequenceFile is the base data structure for the other types of files, like MapFile, SetFile, ArrayFile and BloomMapFile.
</p>
<p>
<img src="images/hdfs-mapfile-index-data-bloom.png"  alt="./images/hdfs-mapfile-index-data-bloom.png" />
</p>
<p>
MapFile是由两个SequenceFile组成，一个是index文件，一个是data文件。data文件里面的key是顺序存储的，index文件是data中key的部分索引. index的key和data的key相同，而value是这个record在data文件中的偏移，至于这个索引间隔可以通过setIndexInterval来设置。操作的时候会将index全部都读取到内存，然后在index里面所二分查找，然后在data文件里面做顺序查找。 #note: 如果data文件要压缩的话，那么这个边界必须和index对应
</p>
<p>
SetFile是基于MapFile完成的，只不过value = NullWritable
</p>
<p>
ArrayFile也是基于MapFile完成的，只不过key = LongWriatble，然后每次写入都会+1
</p>
<p>
BloomMapFile扩展了MapFile添加了一个bloom文件，存储的是DynamicBloomFilter序列化内容。在判断key是否在MapFile之前，先走BloomFilter.
</p>
</div>

</div>

<div id="outline-container-1-5" class="outline-4">
<h4 id="sec-1-5"><span class="section-number-4">1.5</span> The Truth About MapReduce Performance on SSDs</h4>
<div class="outline-text-4" id="text-1-5">

<p><a href="http://blog.cloudera.com/blog/2014/03/the-truth-about-mapreduce-performance-on-ssds/">http://blog.cloudera.com/blog/2014/03/the-truth-about-mapreduce-performance-on-ssds/</a>
</p>
<p>
todo:
</p>
</div>

</div>

<div id="outline-container-1-6" class="outline-4">
<h4 id="sec-1-6"><span class="section-number-4">1.6</span> HDFS: Hadoop and Solid State Drive</h4>
<div class="outline-text-4" id="text-1-6">

<p><a href="http://hadoopblog.blogspot.com/2012/05/hadoop-and-solid-state-drives.html">http://hadoopblog.blogspot.com/2012/05/hadoop-and-solid-state-drives.html</a>
</p>
<p>
todo:
</p>
</div>
</div>

</div>

<div id="outline-container-2" class="outline-3">
<h3 id="sec-2"><span class="section-number-3">2</span> Notes</h3>
<div class="outline-text-3" id="text-2">


</div>

<div id="outline-container-2-1" class="outline-4">
<h4 id="sec-2-1"><span class="section-number-4">2.1</span> HDFS Shell</h4>
<div class="outline-text-4" id="text-2-1">

<ul>
<li>balancer
<ul>
<li>start-balancer.sh
</li>
<li>stop-balancer.sh
</li>
<li>#note: 可以限制比例阈值和传输带宽
</li>
</ul>

</li>
<li>fsck
</li>
</ul>


</div>

</div>

<div id="outline-container-2-2" class="outline-4">
<h4 id="sec-2-2"><span class="section-number-4">2.2</span> Filesystem Corruption and Missing Blocks</h4>
<div class="outline-text-4" id="text-2-2">

<ul>
<li>HadoopRecovery &lt; Storage &lt; TWiki <a href="https://www.opensciencegrid.org/bin/view/Storage/HadoopRecovery">https://www.opensciencegrid.org/bin/view/Storage/HadoopRecovery</a>
</li>
<li>HadoopOperations &lt; Storage &lt; TWiki <a href="https://www.opensciencegrid.org/bin/view/Storage/HadoopOperations">https://www.opensciencegrid.org/bin/view/Storage/HadoopOperations</a>
</li>
</ul>

<p>如果hdfs文件系统出现损坏的话，可以在webpage上面看到报警提示
</p>
<p>
<img src="images/hdfs-filesystem-corruption-and-missing-blocks.png"  alt="./images/hdfs-filesystem-corruption-and-missing-blocks.png" />
</p>
<p>
或者可以通过运行命令hadoop dfsadmin -report看到系统状况
</p>


<pre class="example">dp@dp1:~$ hadoop dfsadmin -report
Configured Capacity: 487173353816064 (443.08 TB)
Present Capacity: 466468596971008 (424.25 TB)
DFS Remaining: 288401443913728 (262.3 TB)
DFS Used: 178067153057280 (161.95 TB)
DFS Used%: 38.17%
Under replicated blocks: 1
Blocks with corrupt replicas: 1
Missing blocks: 1
</pre>


<p>
按照提示可以运行hadoop fsck来检查整个文件系统。首先使用hadoop fsck /察看整个文件系统的状态如何。如果某个文件出现问题的话那么会报告
</p>


<pre class="example">/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563: CORRUPT block blk_6229461233186357508
/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563:  Under replicated blk_6229461233186357508_18529950. Target Replicas is 3 but found 1 replica(s).
</pre>

<p>
说明文件/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563存在问题。
</p>
<p>
我们可以进一步察看这个文件的状态。使用下面的命令 hadoop fsck <i>hbase</i>.corrupt/dp18.umeng.com%3A60020.1349065853563 -files -locations -blocks -racks
</p>


<pre class="example">dp@dp2:~$ hadoop fsck /hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563 -files -locations -blocks -racks
FSCK started by dp (auth:SIMPLE) from /10.18.10.55 for path /hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563 at Mon Oct 08 15:17:07 CST 2012
/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563 66050 bytes, 1 block(s):
/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563: CORRUPT block blk_6229461233186357508
 Under replicated blk_6229461233186357508_18529950. Target Replicas is 3 but found 1 replica(s).
0. blk_6229461233186357508_18529950 len=66050 repl=1 [/default-rack/10.18.10.71:50010]

Status: CORRUPT
 Total size:    66050 B
 Total dirs:    0
 Total files:   1
 Total blocks (validated):      1 (avg. block size 66050 B)
  ********************************
  CORRUPT FILES:        1
  CORRUPT BLOCKS:       1
  ********************************
 Minimally replicated blocks:   1 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       1 (100.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     1.0
 Corrupt blocks:                1
 Missing replicas:              2 (200.0 %)
 Number of data-nodes:          29
 Number of racks:               1
FSCK ended at Mon Oct 08 15:17:07 CST 2012 in 1 milliseconds


The filesystem under path '/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563' is CORRUPT

</pre>



<hr/>

<p>
默认情况下面如果hdfs发现某个block under replicated的话，会自动对这个block做replication的，直到replicaion factor达到需求。但是有时候hdfs也会stuck住。除了重启的话，也可以试试上面链接提到的方法。
</p><ul>
<li>首先将这个文件的rep factor设置为1，hadoop fs -setrep 1 &lt;file&gt;
</li>
<li>然后将这个文件的rep factor修改回3，hadoop fs -setrep 3 &lt;file&gt;
</li>
<li>#note: 不过很悲剧的是，即使我按照这个方法，这个block似乎也没有回复到指定的factor上面
</li>
<li>#note: 不是所有的hdfs file都是使用replication=3的方案的，对于mapreduce提交的jar以及libjars（在/user/&lt;user&gt;/.staging/&lt;jobid&gt;/下面）的，考虑到需要被多个tasktracker同时取到，replication的数目会偏高，通常是10
</li>
</ul>


</div>

</div>

<div id="outline-container-2-3" class="outline-4">
<h4 id="sec-2-3"><span class="section-number-4">2.3</span> 文件系统API</h4>
<div class="outline-text-4" id="text-2-3">

<p>HDFS文件系统的操作步骤主要如下：
</p><ul>
<li>首先通过configuration获得FileSystem实例
</li>
<li>然后通过FileSystem这个实例操作文件系统上的文件
</li>
<li>代码可以参考 <a href="https://github.com/dirtysalt/tomb/blob/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/GetFS.java">com.dirlt.java.hdfs.GetFS</a>
</li>
</ul>


<p>
影响获取到的具体文件系统是fs.default.name这个值，hdfs文件系统API支持下面几个文件系统(不仅限于，只是常用的）
</p><ul>
<li>Local file fs.LocalFileSystem
</li>
<li>HDFS hdfs hdfs.DistributedFileSystem
<ul>
<li>No file update options(record append, etc). all files are write-once.
</li>
<li>Designed for streaming. Random seeks devastate performance.
</li>
</ul>

</li>
<li>HAR(Hadoop Archive) har fs.HarFileSystem
</li>
</ul>


<p>
以 com.dirlt.java.hdfs.GetFS 为例，如果使用java -cp方式运行的话，那么结果如下
</p>


<pre class="example">fs.default.name = file:///
uri = file:///
uri = file:///
</pre>


<p>
而如果以hadoop来运行的话，因为configuration首先会加载conf/core-site.xml里面存在fs.default.name，因此运行结果如下
</p>


<pre class="example">➜  hdfs git:(master) ✗ export HADOOP_CLASSPATH=./target/classes
➜  hdfs git:(master) ✗ hadoop com.dirlt.java.hdfs.GetFS
fs.default.name = hdfs://localhost:9000
uri = hdfs://localhost:9000
uri = file:///
</pre>


<p>
如果指定的URI schema在configuration里面找不到对应实现的话，那么就会使用fs.default.name作为默认的文件系统。
</p>
</div>

</div>

<div id="outline-container-2-4" class="outline-4">
<h4 id="sec-2-4"><span class="section-number-4">2.4</span> 一致性问题</h4>
<div class="outline-text-4" id="text-2-4">

<ul>
<li>hdfs一致性模型是reader不能够读取到当前被write的block，除非writer调用sync强制进行同步
<ul>
<li>FileSystem有下面几个方法需要稍微说明一下 flush,sync,hflush,hsync
</li>
<li>flush是DataOutputStream的virtual method，调用flush会调用底层stream的flush，或许我们可以简单地认为这个实现就是将缓冲区的数据刷到device上面
</li>
<li>sync是FSDataOutputStream特有的，老版本相当是将datanode数据同步到namenode，这样reader就可以读取到当前的block，但是在高版本deprecated
</li>
<li>hflush则是高版本推荐的sync用法
</li>
<li>hsync不仅仅有hflush功能，还能够调用对应的datanode将数据刷到local fs上面。
</li>
<li>#note: 但是似乎不太work. 参考代码 <a href="https://github.com/dirtysalt/tomb/blob/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/TestConsistency.java">com.dirlt.java.hdfs.TestConsistency</a>
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-2-5" class="outline-4">
<h4 id="sec-2-5"><span class="section-number-4">2.5</span> 读写进度</h4>
<div class="outline-text-4" id="text-2-5">

<ul>
<li>hdfs每次将64KB数据写入datanode pipeline的时候都会调用progress.
</li>
<li>对于本地文件系统的话，可以跟进到RawLocalFileSystem.create发现progress这个方法并没有使用。
</li>
<li>对于分布式文件系统的话，可以跟进到DFSClient.DFSOutputStream.DataStreamer在run里面调用progress
<ul>
<li>但是过程似乎有点复杂，所以也不确实是否真的写入64KB才会调用progress
</li>
</ul>

</li>
<li>代码可以参考 <a href="https://github.com/dirtysalt/tomb/blob/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/TestProgress.java">com.dirlt.java.hdfs.TestProgress</a>
</li>
</ul>


</div>

</div>

<div id="outline-container-2-6" class="outline-4">
<h4 id="sec-2-6"><span class="section-number-4">2.6</span> 获取集群运行状况</h4>
<div class="outline-text-4" id="text-2-6">

<ul>
<li>参考代码 <a href="https://github.com/dirtysalt/tomb/blob/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/ClusterSummary.java">com.dirlt.java.hdfs.ClusterSummary</a>
</li>
<li>通过DFSClient可以获取集群运行状况
</li>
</ul>


</div>

</div>

<div id="outline-container-2-7" class="outline-4">
<h4 id="sec-2-7"><span class="section-number-4">2.7</span> All datanodes are bad. Aborting</h4>
<div class="outline-text-4" id="text-2-7">

<p>当时的情况是增加了datanode的处理线程数目但是没有重启regionserver.怀疑原因可能是文件句柄数量不够，重启regionserver之后恢复正常。
</p>



<pre class="example">2013-06-05 03:45:16,866 FATAL org.apache.hadoop.hbase.regionserver.wal.HLog: Could not append. Requesting close of hlog
java.io.IOException: All datanodes 10.11.0.41:50010 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3088)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1900(DFSClient.java:2627)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2799)
</pre>

</div>
</div>
</div>
</div>

<div id="postamble">
<p class="creator"><a href="http://orgmode.org">Org</a> version 7.9.3f with <a href="http://www.gnu.org/software/emacs/">Emacs</a> version 24</p>

</div>
<!-- DISQUS BEGIN --><div id="disqus_thread"></div><script type="text/javascript">/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * *//* required: replace example with your forum shortname  */var disqus_shortname = 'dirlt';var disqus_identifier = 'hdfs.html';var disqus_title = 'hdfs.html';var disqus_url = 'http://dirlt.com/hdfs.html';/* * * DON'T EDIT BELOW THIS LINE * * */(function() {var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><!-- DISQUS END --></body>
</html>
