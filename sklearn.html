<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>sklearn</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="sklearn"/>
<meta name="generator" content="Org-mode"/>
<meta name="author" content="dirtysalt"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style><link rel="shortcut icon" href="css/favicon.ico" /> <link rel="stylesheet" type="text/css" href="css/site.css" />


</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">sklearn</h1>

<ul>
<li>home <a href="http://scikit-learn.org/stable/index.html">http://scikit-learn.org/stable/index.html</a>
</li>
<li>docs <a href="http://scikit-learn.org/stable/documentation.html">http://scikit-learn.org/stable/documentation.html</a>
</li>
<li><a href="images/scikit-learn-ml-map.png">一张图说明如何选择正确算法</a>
</li>
</ul>



<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="sklearn.html#sec-1">1 Overview</a></li>
<li><a href="sklearn.html#sec-2">2 Building Blocks</a></li>
<li><a href="sklearn.html#sec-3">3 Supervised Learning</a>
<ul>
<li><a href="sklearn.html#sec-3-1">3.1 Support Vector Machines</a></li>
<li><a href="sklearn.html#sec-3-2">3.2 Ensemble methods</a></li>
<li><a href="sklearn.html#sec-3-3">3.3 Nearest Neighbors</a></li>
<li><a href="sklearn.html#sec-3-4">3.4 Naive Bayes</a></li>
</ul>
</li>
<li><a href="sklearn.html#sec-4">4 Model selection and evaluation</a>
<ul>
<li><a href="sklearn.html#sec-4-1">4.1 Cross-validation: evaluating estimator performance</a></li>
<li><a href="sklearn.html#sec-4-2">4.2 Grid Search: searching for estimator parameters</a></li>
<li><a href="sklearn.html#sec-4-3">4.3 Pipeline: chaining estimators</a></li>
<li><a href="sklearn.html#sec-4-4">4.4 Model evaluation: quantifying the quality of predictions</a></li>
<li><a href="sklearn.html#sec-4-5">4.5 Model persistence</a></li>
<li><a href="sklearn.html#sec-4-6">4.6 Validation curves: plotting scores to evaluate models</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-3">
<h3 id="sec-1"><span class="section-number-3">1</span> Overview</h3>
<div class="outline-text-3" id="text-1">

<ul>
<li>supervised learning
<ul>
<li>classification # Identifying to which set of categories a new observation belong to.
</li>
<li>regression # Predicting a continuous value for a new example.
</li>
</ul>

</li>
<li>unsupervised learning
<ul>
<li>clustering # Automatic grouping of similar objects into sets.
</li>
<li>dimensionality reduction # Reducing the number of random variables to consider.
</li>
</ul>

</li>
<li>model selection and evaluation # Comparing, validating and choosing parameters and models.
</li>
<li>dataset transformations # Feature extraction and normalization.
</li>
<li>dataset loading utilities
</li>
</ul>


</div>

</div>

<div id="outline-container-2" class="outline-3">
<h3 id="sec-2"><span class="section-number-3">2</span> Building Blocks</h3>
<div class="outline-text-3" id="text-2">

<p><a href="http://scipy-lectures.github.io/">http://scipy-lectures.github.io/</a> sklearn底层使用的三驾马车numpy, scipy, matplotlib.
</p>

<hr/>
<p>
numpy. 数组/矩阵的表示和运算能力. # import numpy as np
</p>
<p>
numpy provides:
</p><ul>
<li>extension package to Python for multi-dimensional arrays
</li>
<li>closer to hardware (efficiency)
</li>
<li>designed for scientific computation (convenience)
</li>
<li>also known as array oriented computing
</li>
</ul>


<p>
array attributes
</p><ul>
<li>ndim # 维度
</li>
<li>shape # 每个维度大小
</li>
<li>dtype # 存储类型
</li>
<li>T # 转置矩阵
</li>
<li>size # 元素个数
</li>
<li>itemsize # 每个元素占用内存大小
</li>
<li>nbytes # 占用内存大小
</li>
</ul>


<p>
index array
</p><ul>
<li>a[d1, d2, &hellip;] # 多维访问
</li>
<li>a[&lt;array&gt;, &hellip;] # fancy indexing
</li>
</ul>



<hr/>
<p>
pylab. 绘图能力 # import pylab as plt
</p>
<p>
这里有许多示例做参考 <a href="http://scipy-lectures.github.io/intro/matplotlib/matplotlib.html#other-types-of-plots-examples-and-exercises">http://scipy-lectures.github.io/intro/matplotlib/matplotlib.html#other-types-of-plots-examples-and-exercises</a>
</p>

<hr/>
<p>
scipy. 复杂数值处理运算能力.
</p>
<p>
The scipy package contains various toolboxes dedicated to common issues in scientific computing. Its different submodules correspond to different applications, such as interpolation, integration, optimization, image processing, statistics, special functions, etc. scipy can be compared to other standard scientific-computing libraries, such as the GSL (GNU Scientific Library for C and C++), or Matlab’s toolboxes. scipy is the core package for scientific routines in Python; it is meant to operate efficiently on numpy arrays, so that numpy and scipy work hand in hand.
</p>
</div>

</div>

<div id="outline-container-3" class="outline-3">
<h3 id="sec-3"><span class="section-number-3">3</span> Supervised Learning</h3>
<div class="outline-text-3" id="text-3">


</div>

<div id="outline-container-3-1" class="outline-4">
<h4 id="sec-3-1"><span class="section-number-4">3.1</span> Support Vector Machines</h4>
<div class="outline-text-4" id="text-3-1">

<p><a href="http://scikit-learn.org/stable/modules/svm.html">http://scikit-learn.org/stable/modules/svm.html</a>
</p>
<p>
svm可以用来做classification, regression以及outliers detection(异常检测).
</p>
<p>
在sklearn里面svm具体分为SVC/SVR和NuSVC/NuSVR. 两者的区别在 <a href="http://scikit-learn.org/stable/modules/svm.html#mathematical-formulation">这里</a> 可以看到，但是差别应该不大："It can be shown that the Nu-SVC formulation is a reparametrization of the C-SVC and therefore mathematically equivalent."
</p>
<p>
classification有三种分类器分别是SVC, NuSVC, LinearSVC. 其中LinearSVC相同于我SVC使用'linear'核方法，区别在于SVC底层使用libsvm, 而LinearSVC则使用liblinear. 另外LinearSVC得到的结果最后也不会返回support<sub>(支持向量)</sub>. 对于多分类问题SVC使用one-vs-one来生成分类器，也就是说需要构造C(n,2)个分类器。LinearSVC使用one-vs-rest来生成分类器，也就是构造n个分类器。LinearSVC也有比较复杂的算法只构造一个分类器就可以进行多分类。regression有两种回归器分别是SVR和NuSVR. classifier和regressor都允许直接输出概率值。用于异常检测是OneClassSVM.
</p>
<p>
kernel函数支持 1.linear 2. polynomial 3. rbf 4. sigmoid(tanh). 对于unbalanced的问题，sklearn实现允许指定 1.class_weight 2.sample_weight. 其中class_weight表示每个class对应的权重，这个在构造classifier时候就需要设置。如果不确定的话就设置成为'auto'。sample_weight则表示每个实例对应的权重，这个可以在调用训练方法fit的时候传入。另外一个比较重要的参数是C(惩罚代价), 通常来说设置成为1.0就够了。但是如果数据中太多噪音的话那么最好减小一些。
</p>
<p>
在计算效率方面，SVM是通过QP来求解的。基于libsvm的实现时间复杂度在O(d * n^2) ~ O(d * n^3)之间，变化取决于如何使用cache. 所以如果我们内存足够的话那么可以调大cache_size来加快计算速度。其中d表示feature大小，如果数据集合比较稀疏的话，那么可以认为d是non-zero的feature平均数量。libsvm处理数据集合大小最好不要超过10k. 相比之下，liblinear的效率则要好得多，可以很容易训练million级别的数据集合。
</p>



<pre class="src src-Python">#!/usr/bin/env python
#coding:utf-8
#Copyright (C) dirlt

from sklearn import datasets
iris = datasets.load_iris()
digits = datasets.load_digits()

from sklearn import svm
from sklearn import cross_validation
from sklearn.metrics import classification_report

clf = svm.SVC(gamma = 0.001, C = 1.0)
# (data, target) = (iris.data, iris.target)
(data, target) = (digits.data, digits.target)
X_tr, X_tt, y_tr, y_tt = cross_validation.train_test_split(data, target, test_size = 0.3, random_state = 0)
clf.fit(X_tr, y_tr)
y_true, y_pred = y_tt, clf.predict(X_tt)
print(classification_report(y_true, y_pred))
</pre>


</div>

</div>

<div id="outline-container-3-2" class="outline-4">
<h4 id="sec-3-2"><span class="section-number-4">3.2</span> Ensemble methods</h4>
<div class="outline-text-4" id="text-3-2">

<p><a href="http://scikit-learn.org/stable/modules/ensemble.html">http://scikit-learn.org/stable/modules/ensemble.html</a>
</p>
<p>
emsemble方法通常分为两类：
</p><ul>
<li>averaging methods. 平均方法，使用不同的算法构建出几个不同的假设然后取平均效果。算法得到的假设都比较好但是容易overfitting, 通过取平均效果降低variance. 通常算法只是作用在部分数据上。这类方法有Bagging, Random Forest等。sklearn提供了bagging meta-estimator允许传入base-estimator来自动做averaging. RF还提供了两个不同版本，另外一个版本在生成决策树选择threshold上也做了随机。
</li>
<li>boosting methods. 增强方法，使用同一个算法不断地修正和迭代然后组合。算法得到的假设一般都比较弱，但是通过组合在一起得到效果比较好的假设。通常算法作用在全部数据上。这类方法有AdaBoost, Gradient Boosting等。sklearn提供的AdaBoost内部base-estimator默认是DecisionTree, 而GBDT内部base-estimator固定就是decision-tree但是允许自定义损失函数。
</li>
</ul>


<p>
使用Decision Tree来做分类和回归时另外一个好处是可以知道每个feature的重要性：位于DecisionTree越高的feature越重要。不过我的理解是这种feature重要性只能用在DecisionTree这种训练方式上。
</p>
<p>
#note: 从下面程序效果上看，GBDT比RF稍微差一些，并且GBDT运行时间要明显长于RF。用iris数据集合的话两者效果差不多。
</p>



<pre class="src src-Python">#!/usr/bin/env python
#coding:utf-8
#Copyright (C) dirlt

from sklearn import datasets
iris = datasets.load_iris()
digits = datasets.load_digits()

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn import cross_validation
from sklearn.metrics import classification_report

# (data, target) = (iris.data, iris.target)
(data, target) = (digits.data, digits.target)
X_tr, X_tt, y_tr, y_tt = cross_validation.train_test_split(data, target, test_size = 0.3, random_state = 0)

print '----------RandomForest----------'
clf = RandomForestClassifier(n_estimators = 100, bootstrap = True, oob_score = True)
clf.fit(X_tr, y_tr)
print 'OOB Score = %.4f' % clf.oob_score_
print 'Feature Importance = %s' % clf.feature_importances_
y_true, y_pred = y_tt, clf.predict(X_tt)
print(classification_report(y_true, y_pred))

print '----------GradientBoosting----------'
clf = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.6, random_state = 0)
clf.fit(X_tr, y_tr)
print 'Feature Importance = %s' % clf.feature_importances_
y_true, y_pred = y_tt, clf.predict(X_tt)
print(classification_report(y_true, y_pred))
</pre>


</div>

</div>

<div id="outline-container-3-3" class="outline-4">
<h4 id="sec-3-3"><span class="section-number-4">3.3</span> Nearest Neighbors</h4>
<div class="outline-text-4" id="text-3-3">

<p><a href="http://scikit-learn.org/stable/modules/neighbors.html">http://scikit-learn.org/stable/modules/neighbors.html</a>
</p>
<p>
NN可以同时用来做监督和非监督学习。其中非监督学习的NN是其他一些学习方法的基础。
</p>
<p>
在实现上sklearn提供了几种算法来寻找最近点：1. brute-force 2. kd-tree 3. ball-tree 4. auto. 其中auto是根据数量大小自动选择算法的。brute-force是采用暴力搜索算法，kd-tree和ball-tree则建立了内部数据结构来加快检索。假设数据维度是d, 数据集合大小是N的话，那么三个算法时间复杂度分别是O(dN), O(d*logN), O(d*logN). 不过如果d过大的话kd-tree会退化称为O(dN).
</p>
<p>
如果数据量比较小的话那么1比2,3要好，所以在实现上kd-tree/ball-tree发现如果数据集合较小的话就会改用brute-force来做。这个阈值称为leaf_size. leaf_size大小会影响到 1. 构建索引时间(反比) 2. 查询时间(合适的leaf_size可以达到最优) 3. 内存大小(反比). 所以尽可能地增大leaf_size但是确保不会影响查询时间。
</p>
<p>
classifier和regressor基本上就是在这些数据结构上做了一层包装。我们可以指定距离函数以及查找到最近点之后的合成函数. 默认距离函数是minkowski(p=2, 也就欧几里得距离), 合成函数包含uniform和distance(和距离成反比). KNeighborsClassifier是选择附近k个点，而RadiusNeighborsClassifier则是选择附近在radius范围内的所有点。另外还有一个NearestCentroid分类器：假设y有k个classes的话，根据这些class归纳为k类并且计算出中心(centroid), 然后判断离哪个中心近就预测哪个class.
</p>



<pre class="src src-Python">#!/usr/bin/env python
#coding:utf-8
#Copyright (C) dirlt

from sklearn import datasets
iris = datasets.load_iris()
digits = datasets.load_digits()

from sklearn.neighbors import KNeighborsClassifier
from sklearn import cross_validation
from sklearn.metrics import classification_report

# (data, target) = (iris.data, iris.target)
(data, target) = (digits.data, digits.target)
X_tr, X_tt, y_tr, y_tt = cross_validation.train_test_split(data, target, test_size = 0.3, random_state = 0)

clf = KNeighborsClassifier(n_neighbors = 10)
clf.fit(X_tr, y_tr)
y_true, y_pred = y_tt, clf.predict(X_tt)
print(classification_report(y_true, y_pred))
</pre>


</div>

</div>

<div id="outline-container-3-4" class="outline-4">
<h4 id="sec-3-4"><span class="section-number-4">3.4</span> Naive Bayes</h4>
<div class="outline-text-4" id="text-3-4">

<p><a href="http://scikit-learn.org/stable/modules/naive_bayes.html">http://scikit-learn.org/stable/modules/naive_bayes.html</a>
</p>
<p>
朴素贝叶斯用于分类问题，其中两项主要工作就是计算 1.P(X|y) 2.P(y). 两者都是通过MLE(maximum likehood estimation)来完成的。P(y)相对来说比较好计算，计算P(X|y)有下面三种办法：
</p><ol>
<li>如果Xi是连续量的话，Gaussian Naive Bayes. 取y=k的所有Xi数据点，假设这个分布服从高斯分布。计算出这个高斯分布的mean和std之后，就可以计算P(X|y=k)。这个模型系数有d * k个。
</li>
<li>如果Xi是离散量的话，Multinomial Naive Bayes. 那么P(X=u|y=k) = P(X=u, y=k) / P(y=k). 这个模型系数有k * &sum; {Xi}个。模型里面还有一个平滑参数。
</li>
<li>进一步如果Xi是(0,1)的话，Bernoulli Naive Bayes. 通常我们需要提供参数binarize，这个方法用来将X转换成为(0,1).
</li>
</ol>





<pre class="src src-Python">#!/usr/bin/env python
#coding:utf-8
#Copyright (C) dirlt

from sklearn import datasets
iris = datasets.load_iris()
digits = datasets.load_digits()

from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn import cross_validation
from sklearn.metrics import classification_report

(data, target) = (iris.data, iris.target)
clf = GaussianNB()
# (data, target) = (digits.data, digits.target)
# clf = MultinomialNB()
X_tr, X_tt, y_tr, y_tt = cross_validation.train_test_split(data, target, test_size = 0.3, random_state = 0)

clf.fit(X_tr, y_tr)
y_true, y_pred = y_tt, clf.predict(X_tt)
print(classification_report(y_true, y_pred))
</pre>


</div>
</div>

</div>

<div id="outline-container-4" class="outline-3">
<h3 id="sec-4"><span class="section-number-3">4</span> Model selection and evaluation</h3>
<div class="outline-text-3" id="text-4">


</div>

<div id="outline-container-4-1" class="outline-4">
<h4 id="sec-4-1"><span class="section-number-4">4.1</span> Cross-validation: evaluating estimator performance</h4>
<div class="outline-text-4" id="text-4-1">

<p><a href="http://scikit-learn.org/stable/modules/cross_validation.html">http://scikit-learn.org/stable/modules/cross_validation.html</a>
</p>
<ul>
<li>使用train_test_split分开training_set和test_set.
</li>
<li>使用k-fold等方式从training_set中分出validation_set做cross_validation.
</li>
<li>使用cross_val_score来进行cross_validation并且计算cross_validation效果.
</li>
</ul>





<pre class="src src-Python">#!/usr/bin/env python
#coding:utf-8
#Copyright (C) dirlt

import numpy as np
from sklearn import cross_validation
from sklearn import datasets
from sklearn import svm

# iris.data.shape = (150, 4); n_samples = 150, n_features = 4
iris = datasets.load_iris()

# &#20998;&#20986;40%&#20316;&#20026;&#27979;&#35797;&#25968;&#25454;&#38598;&#21512;. random_state&#20316;&#20026;&#38543;&#26426;&#31181;&#23376;
X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size = 0.4, random_state = 0)

# &#20551;&#35774;&#36825;&#37324;&#25105;&#20204;&#24050;&#32463;&#23436;&#25104;&#21442;&#25968;&#31354;&#38388;&#25628;&#32034;
clf = svm.SVC(gamma = 0.001, C = 100., kernel = 'linear')
# &#20351;&#29992;cross_validation&#26597;&#30475;&#21442;&#25968;&#25928;&#26524;
scores = cross_validation.cross_val_score(clf, X_train, y_train, cv = 3)
print(<span class="org-string">"Accuracy on cv: %0.2f (+/- %0.2f)"</span> % (scores.mean(), scores.std() * 2))

# &#22914;&#26524;&#25928;&#26524;&#19981;&#38169;&#30340;&#35805;&#65292;&#23601;&#26159;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;&#27169;&#22411;&#35745;&#31639;&#27979;&#35797;&#25968;&#25454;
clf.fit(X_train, y_train)
print(np.mean(clf.predict(X_test) == y_test))
</pre>


</div>

</div>

<div id="outline-container-4-2" class="outline-4">
<h4 id="sec-4-2"><span class="section-number-4">4.2</span> Grid Search: searching for estimator parameters</h4>
<div class="outline-text-4" id="text-4-2">

<p><a href="http://scikit-learn.org/stable/modules/grid_search.html">http://scikit-learn.org/stable/modules/grid_search.html</a>
</p>
<p>
参数空间搜索方式大致分为三类： 1.暴力 2.随机 3.adhoc. 其中23和特定算法相关。
</p>
<p>
我们这里以暴力搜索为例。我们只需要以字典方式提供搜索参数的可选列表即可。因为搜索代码内部会使用cross_validation来做验证，所以我们只需提供cross_validatio参数即可。下面代码摘自这个 <a href="http://scikit-learn.org/stable/auto_examples/grid_search_digits.html">例子</a> 。
</p>



<pre class="src src-Python">#!/usr/bin/env python
#coding:utf-8
#Copyright (C) dirlt

from __future__ import print_function

from sklearn import datasets
from sklearn.cross_validation import train_test_split
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.svm import SVC

# Loading the Digits dataset
digits = datasets.load_digits()

# To apply an classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
(n_samples, h, w) = digits.images.shape
# &#36825;&#37324;&#20063;&#21487;&#20197;&#30452;&#25509;&#29992;digits.data&#21644;digits.target. digits.data&#24050;&#32463;&#26159;reshape&#20043;&#21518;&#32467;&#26524;.
X = digits.images.reshape((n_samples, -1))
y = digits.target

# Split the dataset in two equal parts
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)

# Set the parameters by cross-validation
# &#25552;&#20379;&#21442;&#25968;&#30340;&#21487;&#36873;&#21015;&#34920;
tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

# &#38142;&#25509;&#20013;&#32473;&#30340;&#20195;&#30721;&#36824;&#23545;cross_validation&#25928;&#26524;&#35780;&#20215;&#26041;&#24335;(scoring)&#36827;&#34892;&#20102;&#25628;&#32034;
clf = GridSearchCV(SVC(), tuned_parameters, cv=5) # &#20351;&#29992;k-fold&#21010;&#20998;&#20986;validation_set. k = 5
clf.fit(X_train, y_train)

print(<span class="org-string">"Best parameters set found on development set:"</span>)
print(clf.best_estimator_)
print(<span class="org-string">"Grid scores on development set:"</span>)
for params, mean_score, scores in clf.grid_scores_:
    print(<span class="org-string">"%0.3f (+/-%0.03f) for %r"</span>
        % (mean_score, scores.std() / 2, params))
print(<span class="org-string">"Detailed classification report:"</span>)
print(<span class="org-string">"The model is trained on the full development set."</span>)
print(<span class="org-string">"The scores are computed on the full evaluation set."</span>)
y_true, y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true, y_pred))
</pre>


<p>
代码最后使用最优模型作用在测试数据上，然后使用classification_report打印评分结果.
</p>


<pre class="example">Best parameters set found on development set:
SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.001,
  kernel=rbf, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
Grid scores on development set:
0.986 (+/-0.001) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}
0.963 (+/-0.004) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}
0.989 (+/-0.003) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}
0.985 (+/-0.003) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}
0.989 (+/-0.003) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}
0.983 (+/-0.003) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}
0.989 (+/-0.003) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}
0.983 (+/-0.003) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}
0.976 (+/-0.005) for {'kernel': 'linear', 'C': 1}
0.976 (+/-0.005) for {'kernel': 'linear', 'C': 10}
0.976 (+/-0.005) for {'kernel': 'linear', 'C': 100}
0.976 (+/-0.005) for {'kernel': 'linear', 'C': 1000}
Detailed classification report:
The model is trained on the full development set.
The scores are computed on the full evaluation set.
             precision    recall  f1-score   support

          0       1.00      1.00      1.00        60
          1       0.95      1.00      0.97        73
          2       1.00      0.97      0.99        71
          3       1.00      1.00      1.00        70
          4       1.00      1.00      1.00        63
          5       0.99      0.97      0.98        89
          6       0.99      1.00      0.99        76
          7       0.98      1.00      0.99        65
          8       1.00      0.96      0.98        78
          9       0.97      0.99      0.98        74

avg / total       0.99      0.99      0.99       719
</pre>


</div>

</div>

<div id="outline-container-4-3" class="outline-4">
<h4 id="sec-4-3"><span class="section-number-4">4.3</span> Pipeline: chaining estimators</h4>
<div class="outline-text-4" id="text-4-3">

<p><a href="http://scikit-learn.org/stable/modules/pipeline.html">http://scikit-learn.org/stable/modules/pipeline.html</a>
</p>
<p>
将多个阶段串联起来自动化
</p>
</div>

</div>

<div id="outline-container-4-4" class="outline-4">
<h4 id="sec-4-4"><span class="section-number-4">4.4</span> Model evaluation: quantifying the quality of predictions</h4>
<div class="outline-text-4" id="text-4-4">

<p><a href="http://scikit-learn.org/stable/modules/model_evaluation.html">http://scikit-learn.org/stable/modules/model_evaluation.html</a>
</p>
<p>
There are 3 different approaches to evaluate the quality of predictions of a model: # 有3中不同方式来评价模型预测结果
</p><ol>
<li>Estimator score method: Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve. # 模型自身内部的评价比如损失函数等
</li>
<li>Scoring parameter: Model-evaluation tools using cross-validation (such as cross_validation.cross_val_score and grid_search.GridSearchCV) rely on an internal scoring strategy. # cv的评价，通常是数值表示. 比如'f1'.
</li>
<li>Metric functions: The metrics module implements functions assessing prediction errors for specific purposes. # 作用在测试数据的评价，可以是数值表示，也可以是文本图像等表示. 比如'classification_report'.
</li>
</ol>


<p>
其中23是比较相关的。差别在于3作用在测试数据上是我们需要进一步分析的，所以相对来说评价方式会更多一些。而2还是在模型选择阶段所以我们更加倾向于单一数值表示。
</p>

<hr/>

<p>
sklearn还提供了DummyEstimator. 它只有有限的几种比较dummy的策略，主要是用来给出baseline.
</p>
<p>
DummyClassifier implements three such simple strategies for classification:
</p><ul>
<li>'stratified' generates randomly predictions by respecting the training set’s class distribution,
</li>
<li>'most_frequent' always predicts the most frequent label in the training set,
</li>
<li>'uniform' generates predictions uniformly at random.
</li>
<li>'constant' always predicts a constant label that is provided by the user.
</li>
</ul>


<p>
DummyRegressor also implements three simple rules of thumb for regression:
</p><ul>
<li>'mean' always predicts the mean of the training targets.
</li>
<li>'median' always predicts the median of the training targests.
</li>
<li>'constant' always predicts a constant value that is provided by the user.
</li>
</ul>


</div>

</div>

<div id="outline-container-4-5" class="outline-4">
<h4 id="sec-4-5"><span class="section-number-4">4.5</span> Model persistence</h4>
<div class="outline-text-4" id="text-4-5">

<p><a href="http://scikit-learn.org/stable/modules/model_persistence.html">http://scikit-learn.org/stable/modules/model_persistence.html</a>
</p>
<p>
可以使用python自带的pickle模块，或者是sklearn的joblib模块。joblib相对pickle能更有效地序列化到磁盘上，但缺点是不能够像pickle一样序列化到string上。
</p>
</div>

</div>

<div id="outline-container-4-6" class="outline-4">
<h4 id="sec-4-6"><span class="section-number-4">4.6</span> Validation curves: plotting scores to evaluate models</h4>
<div class="outline-text-4" id="text-4-6">

<p><a href="http://scikit-learn.org/stable/modules/learning_curve.html">http://scikit-learn.org/stable/modules/learning_curve.html</a>
</p>
<p>
Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data. # bias是指模型对不同训练数据的偏差，variance则是指模型对不同训练数据的敏感程度，噪音则是数据自身属性。这三个问题造成预测偏差。
</p>
<p>
#note: 这个特性应该是从0.15才有的。之前我用apt-get安装的sklearn-0.14.1没有learning_curve这个模块。
</p>

<hr/>
<p>
validation curve
</p>
<p>
观察模型某个参数变化对于training_set和validation_set结果影响，来确定是否underfitting或者overfitting. 参考这个 <a href="http://scikit-learn.org/stable/auto_examples/plot_validation_curve.html">例子</a> 绘图
</p>
<p>
If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter gamma on the digits dataset.
</p>
<p>
可以看到gamma在5 * 10<sup>-4</sup>附近cross-validation score开始下滑，但是training score还是不错的，说明overfitting.
</p>
<p>
<img src="images/sklearn-plot-validation-curve.png"  alt="./images/sklearn-plot-validation-curve.png" />
</p>

<hr/>
<p>
learning curve
</p>
<p>
观察增加数据量是否能够改善效果。通常增加数据量会使得traning score和validation score不断收敛。如果两者收敛处score比较低的话(high-bias), 那么增加数据量是不能够改善效果的话，那么我们就需要更换模型。相反如果两者收敛位置score比较高的话，那么增加数据量就可以改善效果。参考这个 <a href="http://scikit-learn.org/stable/auto_examples/plot_learning_curve.html">例子</a> 绘图
</p>
<p>
第一幅图是是用朴素贝叶斯的learning curve. 可以看到high-bias情况。第二幅图是使用SVM(RBF kernel)的learning curve. 学习情况明显比朴素贝叶斯要好。
</p>
<p>
<img src="images/sklearn-plot-learning-curve-001.png"  alt="./images/sklearn-plot-learning-curve-001.png" /> <img src="images/sklearn-plot-learning-curve-002.png"  alt="./images/sklearn-plot-learning-curve-002.png" />
</p></div>
</div>
</div>
</div>

<div id="postamble">
<p class="creator"><a href="http://orgmode.org">Org</a> version 7.9.3f with <a href="http://www.gnu.org/software/emacs/">Emacs</a> version 24</p>

</div>
<!-- DISQUS BEGIN --><div id="disqus_thread"></div><script type="text/javascript">/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * *//* required: replace example with your forum shortname  */var disqus_shortname = 'dirlt';var disqus_identifier = 'sklearn.html';var disqus_title = 'sklearn.html';var disqus_url = 'http://dirlt.com/sklearn.html';/* * * DON'T EDIT BELOW THIS LINE * * */(function() {var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><!-- DISQUS END --></body>
</html>
