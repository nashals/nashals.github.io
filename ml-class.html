<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head><!-- <meta name="baidu-site-verification" content="707024a76f8f40b549f07f478abab237"/> -->
<title>ml-class</title>
<meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"/>
<meta name="title" content="ml-class"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2014-12-04T15:39+0800"/>
<meta name="author" content="dirtysalt"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style><link rel="shortcut icon" href="css/favicon.ico" /> <link rel="stylesheet" type="text/css" href="css/site.css" />


</head>
<body><!-- <div id="bdshare" class="bdshare_t bds_tools_32 get-codes-bdshare"><a class="bds_tsina"></a><span class="bds_more"></span><a class="shareCount"></a></div> --><!-- Place this tag where you want the +1 button to render --><!-- <g:plusone annotation="inline"></g:plusone> -->

<div id="preamble">

</div>

<div id="content">
<h1 class="title">ml-class</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="ml-class.html#sec-1">1 ml-class</a>
<ul>
<li><a href="ml-class.html#sec-1-1">1.1 definition</a></li>
<li><a href="ml-class.html#sec-1-2">1.2 prerequisite</a></li>
<li><a href="ml-class.html#sec-1-3">1.3 supervised learning</a></li>
<li><a href="ml-class.html#sec-1-4">1.4 neural networks</a></li>
<li><a href="ml-class.html#sec-1-5">1.5 support vector machine</a></li>
<li><a href="ml-class.html#sec-1-6">1.6 advice for applying ML</a></li>
<li><a href="ml-class.html#sec-1-7">1.7 unsupervised learning</a></li>
<li><a href="ml-class.html#sec-1-8">1.8 dimensionality reduction</a></li>
<li><a href="ml-class.html#sec-1-9">1.9 anomaly detection</a></li>
<li><a href="ml-class.html#sec-1-10">1.10 recommender system</a></li>
<li><a href="ml-class.html#sec-1-11">1.11 ML in large scale</a></li>
<li><a href="ml-class.html#sec-1-12">1.12 appendix code</a>
<ul>
<li><a href="ml-class.html#sec-1-12-1">1.12.1 feature normalization</a></li>
<li><a href="ml-class.html#sec-1-12-2">1.12.2 linear regression cost function</a></li>
<li><a href="ml-class.html#sec-1-12-3">1.12.3 neural network cost function</a></li>
<li><a href="ml-class.html#sec-1-12-4">1.12.4 pca(principal compoenent analysis)</a></li>
<li><a href="ml-class.html#sec-1-12-5">1.12.5 gaussian distribution</a></li>
<li><a href="ml-class.html#sec-1-12-6">1.12.6 anomaly detection select threshold</a></li>
<li><a href="ml-class.html#sec-1-12-7">1.12.7 collaborative filtering cost function</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> ml-class</h2>
<div class="outline-text-2" id="text-1">

<p><a href="https://class.coursera.org/ml-007">https://class.coursera.org/ml-007</a>
</p>

</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> definition</h3>
<div class="outline-text-3" id="text-1-1">

<p>T(task),E(experience),P(performance)
</p>
</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> prerequisite</h3>
<div class="outline-text-3" id="text-1-2">

<ul>
<li>linear algebra
<ul>
<li>matrices / vectors # addition / subtraction/ multiplication / inversion / transposition
</li>
<li>some matrices are not invertible called singular / degenerate # redundant(linear dependent) or too many features
</li>
<li>pseudo inverse(pinv, works on matrix non-invertible) and inverse(inv)
</li>
</ul>

</li>
<li>Octave # todo(dirlt)
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> supervised learning</h3>
<div class="outline-text-3" id="text-1-3">

<ul>
<li><a href="http://math.stackexchange.com/questions/141381/regression-vs-classification">http://math.stackexchange.com/questions/141381/regression-vs-classification</a>
</li>
<li>regression # continuous valued output.
<ul>
<li>squared error function (as cost function)
</li>
<li>linear regression
</li>
<li>polynomial regression
</li>
</ul>

</li>
<li>classification # discrete valued output.
<ul>
<li>decision boundary
</li>
<li>binary-class(negative/positive) vs. multi-class
</li>
<li>logistic regression # h(x) in [0,1]
<ul>
<li>sigmoid / logistic function
</li>
<li>g(x) = 1 / (1 + e^-x)
</li>
<li>interpretation of hypothesis output.
</li>
<li>p(y=0|x;theta) + p(y=1|x;theta) = 1 (for binary-class)
</li>
<li>cost funciton = [-log(h(x)) if y = 1, -log(1-h(x)) if y = 0]
</li>
<li>=&gt; -(log(h(x)) * y + log(1-h(x)) * (1-y))
</li>
</ul>

</li>
<li>SVM(support vector machine)
</li>
</ul>

</li>
<li>training set / historical data set.
<ul>
<li>input variables / features
<ul>
<li>univariate # single input variable
</li>
<li>multivariate # multiple input variables.
</li>
<li>feature scaling / mean normalization
</li>
</ul>

</li>
<li>output variables / targets
</li>
<li>feature scaling (otherwise more steps to find global minimum), approximately [-1,1]
</li>
</ul>

</li>
<li>hypothesis parameters(theta) and hypothesis(theta * x)
</li>
<li>cost function
<ul>
<li>convex and non-convex function
</li>
<li>"batch" = uses all training set
</li>
<li>gradient descent algorithm
<ul>
<li>learning rate, derivative term.
</li>
<li>if learning rate is too small, converge rate could be low.
</li>
<li>if learning rate is too large, fail to converge or even diverge.
</li>
</ul>

</li>
<li>gradient checking
</li>
<li>optimization algorithm: conjugate gradient / BFGS / L-BFGS
<ul>
<li>no need to manually peek learning rate
</li>
<li>faster than gradient descent
</li>
<li>provided cost function and partial derivatives
</li>
<li>'fmincg' or 'fminunc' in Octave
</li>
</ul>

</li>
</ul>

</li>
<li>overfitting
<ul>
<li>problem
<ul>
<li>if underfit -&gt; high bias
</li>
<li>if overfit -&gt; high variance
</li>
<li>not generalize new examples
</li>
</ul>

</li>
<li>addressing
<ul>
<li>reduce number of features.
<ul>
<li>manually select which features to keep
</li>
<li>model selection algorithm
</li>
</ul>

</li>
<li>regularization
<ul>
<li>keep all features but reduce magnitude/values of parameters
</li>
<li>works well when we have a lot of features
</li>
<li>if regularization parameter is very large -&gt; underfitting.
</li>
</ul>

</li>
</ul>

</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> neural networks</h3>
<div class="outline-text-3" id="text-1-4">

<ul>
<li><a href="images/neural-network-cost-function-andrew-ng.pdf">Andrew Ng's Lecture PDF</a>
</li>
<li>motivation
<ul>
<li>complex non-linear classification / hypothesis
</li>
<li>many features -&gt; too many polynomial terms.
<ul>
<li>quantic x, then O(n^x).
</li>
<li>quadratic : O(n ^ 2)
</li>
<li>cubic: O(n ^ 3)
</li>
</ul>

</li>
</ul>

</li>
<li>background
<ul>
<li>origins: algorithms that try to mimic the brain
</li>
<li>widely used in 80s and early 90s, popularity diminished in late 90s
</li>
<li>recent resurgence
</li>
<li>"one learning algorithm" hypothesis = cortex.
</li>
</ul>

</li>
<li>model representation
<ul>
<li>neuron in the brain
<ul>
<li>dendrite = input write
</li>
<li>axon = output write
</li>
<li>cell body / nucleus
</li>
<li>communicated by spike(pulse of electricity)
</li>
</ul>

</li>
<li>neuron model: logistic unit
<ul>
<li>sigmoid (logistic) activation function
</li>
<li>hypothesis parameter = weight
</li>
</ul>

</li>
<li>layer: input/output/hidden
<ul>
<li>a(i,j) = "activation" of unit i in layer j
</li>
<li>theta(j) = matrix of weights controlling function mapping from layer j to layer j+1
</li>
<li>if network has s(j) units in layer j, and s(j+1) units in layer j+1, then theta(j) is M(s(j+1), s(j)+1)
</li>
</ul>

</li>
<li>forward propagation
</li>
<li>backward propagation
</li>
</ul>

</li>
<li>backpropagation algorithm
<ul>
<li>general cost function
</li>
<li>delta(j,l) = "error" of node j in layer l
</li>
<li>intuition # use backpropagation algorithm to compute derivatives.
</li>
<li>implementation
<ul>
<li>unroll parameters
</li>
<li>gradient checking(inefficient) to verify backprop derivatives
</li>
<li>initialize parameters randomly[symmetry breaking] (otherwise features are duplicated)
</li>
</ul>

</li>
</ul>

</li>
<li>putting together
<ul>
<li>network architecture
<ul>
<li>no. of input units: dimension of features
</li>
<li>no. of output units: number of classes
</li>
<li>hidden layer
<ul>
<li>reasonable default: 1 hidden layer, or &gt;1 hidden layer have same no. of hidden units in every layer(usually the more the better)
</li>
<li>no. of hidden units = [2,3,4] * no. input units.
</li>
</ul>

</li>
<li>network size
<ul>
<li>small # fewer parameters, more prone to underfitting, computationally cheaper.
</li>
<li>large # more parameters, more prone to overfitting, computationally more expensive.
</li>
</ul>

</li>
</ul>

</li>
<li>training a neural network
<ul>
<li>randomly initialize weights
</li>
<li>for-loop to iterate each training samples.
</li>
<li>forward propagation to compute activation
</li>
<li>compute cost function
</li>
<li>backward propagation to compute partial derivatives
</li>
<li>gradient checking
</li>
<li>gradient descent algorithm
</li>
</ul>

</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-5" class="outline-3">
<h3 id="sec-1-5"><span class="section-number-3">1.5</span> support vector machine</h3>
<div class="outline-text-3" id="text-1-5">

<ul>
<li>alternative view of logistic regression
</li>
<li>SVM cost function # replace sigmoid function with two simple functions (cost0 and cost1)
<ul>
<li>cost function = -y * cost1(tx) + (1-y) * cost0(tx)
</li>
<li>hypothesis: y = 1 if tx &gt;=0. y = 0 otherwise.
</li>
</ul>

</li>
<li>SVM decision boundary / large margin intuition (if C very large)
</li>
<li>kernel / kernel function
<ul>
<li>for more features
</li>
<li>to compute similarity (with landmarks) as more complex, non-linear features.
</li>
<li>gaussian kernel function.
<ul>
<li>K(x,y,e) = exp ^ (-0.5 / e^2 * |x-y|^2)
</li>
<li>if e^2 is large, high bias and low variance
</li>
<li>if e^2 is small, low bias and high variance
</li>
</ul>

</li>
<li>output range [0,1]
</li>
</ul>

</li>
<li>how it works
<ul>
<li>choose typical landmarks.
</li>
<li>compute similarity with landmarks as input [0,1]
</li>
<li>translate into a typical classifier problem.
</li>
<li>number of features == number of landmarks.
</li>
</ul>

</li>
<li>practice
<ul>
<li>liblinear, libsvm
</li>
<li>specify 1) choice of parameter C 2) kernel function
</li>
<li>no kernel / linear kernel function # n &gt;&gt; m
</li>
<li>gaussian kernel function # m &gt;&gt; n
</li>
<li>polynomial kernel function
</li>
<li>string kernel / chi-square kernel / histogram intersection kernel
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-6" class="outline-3">
<h3 id="sec-1-6"><span class="section-number-3">1.6</span> advice for applying ML</h3>
<div class="outline-text-3" id="text-1-6">

<ul>
<li>unacceptablely large errors in its predictions
<ul>
<li>don't just use gut feelings and do the following things randomly
</li>
<li>get more training examples. (but not the more the better) =&gt; fix high variance
</li>
<li>try smaller sets of features. =&gt; fix high variance
</li>
<li>try getting additional features. =&gt; fix high bias
</li>
<li>try polynomial features. =&gt; fix high bias
</li>
<li>try decreasing/increasing lambda. =&gt; fix high bias/variance
</li>
</ul>

</li>
<li>system diagnostics
<ul>
<li>evaluating hypothesis
<ul>
<li>split examples randomly into training set(70%) and test set(30%).
</li>
<li>see J_test(theta) is overfitting or not.
</li>
</ul>

</li>
<li>model selection (for choosing polynomial terms and regularization)
<ul>
<li>split examples randomly into training set(60%), cross validation set(20%), and test set(20%)
</li>
<li>use cross validation set to select model, and get estimate of generalization error.
</li>
</ul>

</li>
<li>high bias vs. variance
<ul>
<li>bias =&gt; underfit: J_train(theta) is high, J_cv/test(theta) = J_train(theta)
</li>
<li>variance =&gt; overfit: J_train(theta) is low, but J_cv/test(theta) &gt; J_train(theta)
</li>
<li>learnin curves # J_cv/test(theta) and J_train(theta) over training set size
</li>
<li>if suffers from high bias, more training data will not help
</li>
<li>if suffers from high variance, more training data might help
</li>
</ul>

</li>
</ul>

</li>
<li>numerical evaluation # a real number tells how well is your system.
</li>
<li>error analysis # spot any systematic trend in what type of examples it is making errors on
</li>
<li>skewed classes.
<ul>
<li>y = 1 in presence of rare class
</li>
<li>precision = true positive / [no. of predicted positive = (true pos + false pos)]
</li>
<li>recall = true positive / [no. of actual positive = (true pos + false neg)]
</li>
<li>good classifier: precision and recall are both high enough.
<ul>
<li>but there are tradeoffs between both
</li>
<li>F score = 2 * P * R / (P + R)
</li>
<li>note: see "anomaly detection select threshold" how to compute P,R, and F.
</li>
</ul>

</li>
</ul>

</li>
<li>large data rationale
<ul>
<li>assume features have sufficient information to predicate accurately
</li>
<li>useful test: give the input x, can a human expert confidently predict y?
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-7" class="outline-3">
<h3 id="sec-1-7"><span class="section-number-3">1.7</span> unsupervised learning</h3>
<div class="outline-text-3" id="text-1-7">

<ul>
<li>cluster algorithm
</li>
<li>cocktail party problem
</li>
<li>K-means algorithm
<ul>
<li>cluster centroid
</li>
<li>K = cluster number, k = cluster index
<ul>
<li>should have K &lt; m
</li>
<li>choose K manually(most time) or with elbow method
</li>
</ul>

</li>
<li>objective function = distances between training set and centroids.
<ul>
<li>convex, but risk of local optima
</li>
<li>randomly choose centroids from training set.
</li>
<li>multiple random initialization
</li>
</ul>

</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-8" class="outline-3">
<h3 id="sec-1-8"><span class="section-number-3">1.8</span> dimensionality reduction</h3>
<div class="outline-text-3" id="text-1-8">

<ul>
<li>motivation
<ul>
<li>data compression
</li>
<li>data visualization
</li>
<li>speed up learning algorithm
</li>
</ul>

</li>
<li>PCA(principal component analysis)
<ul>
<li>find k vectors onto which to project the data
</li>
<li>minimize the projection error(different to linear regression)
</li>
<li>algorithm # reduce n dimensions to k dimensions
<ul>
<li>sigma = 1/m * sum{X(i) * X(i)'}. X(i)~n*1, so sigma~n*n
</li>
<li>[U,S,V] = svd(sigma) # singular value decomposition
</li>
<li>U~n*n. use first k columns called U_reduce~(n*k)
</li>
<li>z = U_reduce' * X(i) ~ (k * n * n * 1) = (k*1)
</li>
<li>reconstruct: X_approx(i) = U_reduce * z ~ (n * k * k * 1) = (n*1)
</li>
</ul>

</li>
<li>choose k # n% of variance is retained.
<ul>
<li>n = sum{i=1,k}S<sub>ii</sub> / sum{i=1,n}S<sub>ii</sub> (S from svd, diagonal matrix)
</li>
<li>n = 99 typical value
</li>
</ul>

</li>
</ul>

</li>
<li>comments
<ul>
<li>don't use PCA to prevent overfitting
</li>
<li>use raw data first, then consider PCA
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-9" class="outline-3">
<h3 id="sec-1-9"><span class="section-number-3">1.9</span> anomaly detection</h3>
<div class="outline-text-3" id="text-1-9">

<ul>
<li>gaussian distribution
<ul>
<li>X ~ N(u, e^2) # X distributed as N. where mean = u, variance = e^2
</li>
<li>p(x, u, e^2) = 1 / ((sqrt(2 * pi) * e)) * exp ^ { - (x-u)^2 / (2 * e^2) } # probability
</li>
<li>multivariate version
<ul>
<li>to capture anomalous combination of values. computationally expensive.
</li>
<li>u~{n*1}, e~{n*n} (covariance matrix) # intuition. contour not axis aligned.
</li>
<li>p(x, u, e) = 1 / ((2 * pi) ^ (n/2) * sqrt(det(e))) * exp ^ {-0.5 * (x-u)' * e^-1 * (x-u)}
</li>
<li>u = 1/m * sum{x}, e = 1/m * sum{(x-u) * (x-u)'}
</li>
<li>note: m &gt; n, otherwise e is non-invertible.
</li>
</ul>

</li>
</ul>

</li>
<li>how it works
<ul>
<li>model p(x) from data
</li>
<li>p(x) &lt; epsilon to decide if anomalous
<ul>
<li>epsilon # p(x) is comparable for normal and anomalous examples.
</li>
<li>features to distinguish normal and anomalous examples.
</li>
<li>p(x) = p1(x1, u1, e1^2) * &hellip; pj(xj, uj, ej^2).. # j = # of features.
</li>
<li>if xj is not gaussian feature, transform it to fit into gaussian distribution.
</li>
</ul>

</li>
</ul>

</li>
<li>vs. supervised learning
<ul>
<li>anomaly detection
<ul>
<li># of positive cases is very small, while # of negative cases is very large
</li>
<li>many different types of "anomaly", hard to learn from positive cases what anomalies looks like
</li>
<li>future anomalies maybe very different to current ones.
</li>
<li>fraud detection, manufacturing, monitoring machines.
</li>
</ul>

</li>
<li>supervised learning
<ul>
<li># of positive cases and negative cases are both very large
</li>
<li>enough positive cases to learn what positive cases look like
</li>
<li>future positive cases are similar to current ones.
</li>
<li>email spam, weather prediction, cancer classification.
</li>
</ul>

</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-10" class="outline-3">
<h3 id="sec-1-10"><span class="section-number-3">1.10</span> recommender system</h3>
<div class="outline-text-3" id="text-1-10">

<ul>
<li>content based recommendation
</li>
<li>collaborative filtering algorithm
<ul>
<li>low rank matrix factorization
</li>
<li>random initialization to break symmetry
</li>
<li>content features to compute similarity between items
</li>
<li>mean normalization # assign mean value to null
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-11" class="outline-3">
<h3 id="sec-1-11"><span class="section-number-3">1.11</span> ML in large scale</h3>
<div class="outline-text-3" id="text-1-11">

<ul>
<li>stochastic gradient descent algorithm
<ul>
<li>vs. batch gradient descent
</li>
<li>randomly shuffle dataset
</li>
<li>repeat for i = 1..m { for j = 0..n  { update theta_j only use ith data } }
</li>
<li>move to global minimum generally, but not always in one iteration.
</li>
<li>convergence checking
<ul>
<li>use averaged last k(say 1000) examples.
</li>
<li>the larger k, the smoother cost function curve.
</li>
<li>can slowly decrease learning rate over time for convergence.
</li>
</ul>

</li>
</ul>

</li>
<li>mini-batch gradient descent algorithm
<ul>
<li>between batch and stochastic gradient descent
</li>
<li>use b(say 10) examples in one iteration
</li>
<li>take advantage of vectorization
</li>
</ul>

</li>
<li>online learning
</li>
<li>map-reduce and data parallelism
</li>
<li>more data
<ul>
<li>collect from multiple sources
</li>
<li>artificial data synthesis
</li>
</ul>

</li>
<li>ceiling analysis
</li>
</ul>


</div>

</div>

<div id="outline-container-1-12" class="outline-3">
<h3 id="sec-1-12"><span class="section-number-3">1.12</span> appendix code</h3>
<div class="outline-text-3" id="text-1-12">


</div>

<div id="outline-container-1-12-1" class="outline-4">
<h4 id="sec-1-12-1"><span class="section-number-4">1.12.1</span> feature normalization</h4>
<div class="outline-text-4" id="text-1-12-1">




<pre class="src src-Octave">function [X_norm, mu, sigma] = featureNormalize(X)
%FEATURENORMALIZE Normalizes the features in X
%   FEATURENORMALIZE(X) returns a normalized version of X where
%   the mean value of each feature is 0 and the standard deviation
%   is 1. This is often a good preprocessing step to do when
%   working with learning algorithms.

mu = mean(X);
X_norm = bsxfun(@minus, X, mu);

sigma = std(X_norm);
X_norm = bsxfun(@rdivide, X_norm, sigma);


% ============================================================

end
</pre>


</div>

</div>

<div id="outline-container-1-12-2" class="outline-4">
<h4 id="sec-1-12-2"><span class="section-number-4">1.12.2</span> linear regression cost function</h4>
<div class="outline-text-4" id="text-1-12-2">

<p>note(dirlt): works for polynomial regression too.
</p>



<pre class="src src-Octave">function [J, grad] = linearRegCostFunction(X, y, theta, lambda)
%LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear
%regression with multiple variables
%   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the
%   cost of using theta as the parameter for linear regression to fit the
%   data points in X and y. Returns the cost in J and the gradient in grad

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly
J = 0;
grad = zeros(size(theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost and gradient of regularized linear
%               regression for a particular choice of theta.
%
%               You should set J to the cost and grad to the gradient.
%

diff = X * theta - y;
J = sum(diff .^ 2) * 0.5 / m;
t = theta;
t(1) = 0;
J += sum(t .^ 2) * lambda * 0.5 / m;
grad = ((X' * diff) + lambda * t) / m;

% =========================================================================

grad = grad(:);

end
</pre>


</div>

</div>

<div id="outline-container-1-12-3" class="outline-4">
<h4 id="sec-1-12-3"><span class="section-number-4">1.12.3</span> neural network cost function</h4>
<div class="outline-text-4" id="text-1-12-3">

<p><img src="images/neural-network-cost-function.png"  alt="./images/neural-network-cost-function.png" />
</p>
<p>
<img src="images/neural-network-backprop.png"  alt="./images/neural-network-backprop.png" />
</p>



<pre class="src src-Octave">function [J grad] = nnCostFunction(nn_params, ...
                                   input_layer_size, ...
                                   hidden_layer_size, ...
                                   num_labels, ...
                                   X, y, lambda)
%NNCOSTFUNCTION Implements the neural network cost function for a two layer
%neural network which performs classification
%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...
%   X, y, lambda) computes the cost and gradient of the neural network. The
%   parameters for the neural network are <span class="org-string">"unrolled"</span> into the vector
%   nn_params and need to be converted back into the weight matrices.
%
%   The returned parameter grad should be a <span class="org-string">"unrolled"</span> vector of the
%   partial derivatives of the neural network.
%

% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices
% for our 2 layer neural network
Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                 hidden_layer_size, (input_layer_size + 1));

Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                 num_labels, (hidden_layer_size + 1));

% Setup some useful variables
m = size(X, 1);

% You need to return the following variables correctly
J = 0;
Theta1_grad = zeros(size(Theta1));
Theta2_grad = zeros(size(Theta2));

% ====================== YOUR CODE HERE ======================
% Instructions: You should complete the code by working through the
%               following parts.
%
% Part 1: Feedforward the neural network and return the cost in the
%         variable J. After implementing Part 1, you can verify that your
%         cost function computation is correct by verifying the cost
%         computed in ex4.m
%
% Part 2: Implement the backpropagation algorithm to compute the gradients
%         Theta1_grad and Theta2_grad. You should return the partial derivatives of
%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and
%         Theta2_grad, respectively. After implementing Part 2, you can check
%         that your implementation is correct by running checkNNGradients
%
%         Note: The vector y passed into the function is a vector of labels
%               containing values from 1..K. You need to map this vector into a
%               binary vector of 1's and 0's to be used with the neural network
%               cost function.
%
%         Hint: We recommend implementing backpropagation using a for-loop
%               over the training examples if you are implementing it for the
%               first time.
%
% Part 3: Implement regularization with the cost function and gradients.
%
%         Hint: You can implement this around the code for
%               backpropagation. That is, you can compute the gradients for
%               the regularization separately and then add them to Theta1_grad
%               and Theta2_grad from Part 2.
%

X2 = [ones(m, 1)  X];
tx2 = X2 * Theta1';
hx2 = sigmoid(tx2);
X3 = [ones(m, 1) hx2];
tx3 = X3 * Theta2';
hx3 = sigmoid(tx3);
hy = zeros(m, num_labels);
for i = [1:m],
    hy(i, y(i)) = 1;
end;
J = sum(sum(log(hx3) .* (-hy) - log(1 - hx3) .* (1 - hy))) / m;

R = 0;
R += sum(sum(Theta1(:, 2:end) .^ 2));
R += sum(sum(Theta2(:, 2:end) .^ 2));
R *= lambda / m * 0.5;

J += R;

% -------------------------------------------------------------

d3 = hx3 - hy; # M * K
d2 = (d3 * Theta2)(:,2:end) .* sigmoidGradient(tx2); # M * H
Theta2_grad = d3' * X3 / m; # K * M * M * (H+1) = K * (H+1)
Theta1_grad = d2' * X2 / m; # H * M * M * (N+1) = H * (N+1)

t2 = Theta2;
t2(:,1) = 0;
t1 = Theta1;
t1(:,1) = 0;
Theta2_grad += t2 * lambda / m;
Theta1_grad += t1 * lambda / m;

% =========================================================================

% Unroll gradients
grad = [Theta1_grad(:) ; Theta2_grad(:)];


end
</pre>

</div>

</div>

<div id="outline-container-1-12-4" class="outline-4">
<h4 id="sec-1-12-4"><span class="section-number-4">1.12.4</span> pca(principal compoenent analysis)</h4>
<div class="outline-text-4" id="text-1-12-4">




<pre class="src src-Octave">function [U, S] = pca(X)
%PCA Run principal component analysis on the dataset X
%   [U, S, X] = pca(X) computes eigenvectors of the covariance matrix of X
%   Returns the eigenvectors U, the eigenvalues (on diagonal) in S
%

% Useful values
[m, n] = size(X);

% You need to return the following variables correctly.
U = zeros(n);
S = zeros(n);

% ====================== YOUR CODE HERE ======================
% Instructions: You should first compute the covariance matrix. Then, you
%               should use the <span class="org-string">"svd"</span> function to compute the eigenvectors
%               and eigenvalues of the covariance matrix.
%
% Note: When computing the covariance matrix, remember to divide by m (the
%       number of examples).
%

sigma = 1.0 / m * X' * X;
[U,S,_ ] = svd(sigma);



% =========================================================================

end

</pre>


<p>
projectData
</p>


<pre class="src src-Octave">function Z = projectData(X, U, K)
%PROJECTDATA Computes the reduced data representation when projecting only
%on to the top k eigenvectors
%   Z = projectData(X, U, K) computes the projection of
%   the normalized inputs X into the reduced dimensional space spanned by
%   the first K columns of U. It returns the projected examples in Z.
%

% You need to return the following variables correctly.
Z = zeros(size(X, 1), K);

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the projection of the data using only the top K
%               eigenvectors in U (first K columns).
%               For the i-th example X(i,:), the projection on to the k-th
%               eigenvector is given as follows:
%                    x = X(i, :)';
%                    projection_k = x' * U(:, k);
%

U_reduce = U(:, 1:K);

Z = X * U_reduce;



% =============================================================

end

</pre>


<p>
recoverData
</p>


<pre class="src src-Octave">function X_rec = recoverData(Z, U, K)
%RECOVERDATA Recovers an approximation of the original data when using the
%projected data
%   X_rec = RECOVERDATA(Z, U, K) recovers an approximation the
%   original data that has been reduced to K dimensions. It returns the
%   approximate reconstruction in X_rec.
%

% You need to return the following variables correctly.
X_rec = zeros(size(Z, 1), size(U, 1));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the approximation of the data by projecting back
%               onto the original space using the top K eigenvectors in U.
%
%               For the i-th example Z(i,:), the (approximate)
%               recovered data for dimension j is given as follows:
%                    v = Z(i, :)';
%                    recovered_j = v' * U(j, 1:K)';
%
%               Notice that U(j, 1:K) is a row vector.
%

U_reduce = U(:, 1:K);

X_rec = Z * U_reduce';

% =============================================================

end

</pre>

</div>

</div>

<div id="outline-container-1-12-5" class="outline-4">
<h4 id="sec-1-12-5"><span class="section-number-4">1.12.5</span> gaussian distribution</h4>
<div class="outline-text-4" id="text-1-12-5">

<p>compute mean and variance of X
</p>



<pre class="src src-Octave">function [mu sigma2] = estimateGaussian(X)
%ESTIMATEGAUSSIAN This function estimates the parameters of a
%Gaussian distribution using the data in X
%   [mu sigma2] = estimateGaussian(X),
%   The input X is the dataset with each n-dimensional data point in one row
%   The output is an n-dimensional vector mu, the mean of the data set
%   and the variances sigma^2, an n x 1 vector
%

% Useful variables
[m, n] = size(X);

% You should return these values correctly
mu = zeros(n, 1);
sigma2 = zeros(n, 1);

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the mean of the data and the variances
%               In particular, mu(i) should contain the mean of
%               the data for the i-th feature and sigma2(i)
%               should contain variance of the i-th feature.
%

mu = mean(X)';
# xu = X - mu';
# sigma2 = 1.0 / m * sum(xu .^ 2)';
sigma2 = (m-1) / m * var(X)';

% =============================================================


end
</pre>


<p>
compute probability
</p>


<pre class="src src-Octave">function p = multivariateGaussian(X, mu, Sigma2)
%MULTIVARIATEGAUSSIAN Computes the probability density function of the
%multivariate gaussian distribution.
%    p = MULTIVARIATEGAUSSIAN(X, mu, Sigma2) Computes the probability
%    density function of the examples X under the multivariate gaussian
%    distribution with parameters mu and Sigma2. If Sigma2 is a matrix, it is
%    treated as the covariance matrix. If Sigma2 is a vector, it is treated
%    as the \sigma^2 values of the variances in each dimension (a diagonal
%    covariance matrix)
%

k = length(mu);

if (size(Sigma2, 2) == 1) || (size(Sigma2, 1) == 1)
    Sigma2 = diag(Sigma2);
end

X = bsxfun(@minus, X, mu(:)');
p = (2 * pi) ^ (- k / 2) * det(Sigma2) ^ (-0.5) * ...
    exp(-0.5 * sum(bsxfun(@times, X * pinv(Sigma2), X), 2));

end
</pre>


</div>

</div>

<div id="outline-container-1-12-6" class="outline-4">
<h4 id="sec-1-12-6"><span class="section-number-4">1.12.6</span> anomaly detection select threshold</h4>
<div class="outline-text-4" id="text-1-12-6">




<pre class="src src-Octave">function [bestEpsilon bestF1] = selectThreshold(yval, pval)
%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting
%outliers
%   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best
%   threshold to use for selecting outliers based on the results from a
%   validation set (pval) and the ground truth (yval).
%

bestEpsilon = 0;
bestF1 = 0;
F1 = 0;

stepsize = (max(pval) - min(pval)) / 1000;
for epsilon = min(pval):stepsize:max(pval)

    % ====================== YOUR CODE HERE ======================
    % Instructions: Compute the F1 score of choosing epsilon as the
    %               threshold and place the value in F1. The code at the
    %               end of the loop will compare the F1 score for this
    %               choice of epsilon and set it to be the best epsilon if
    %               it is better than the current choice of epsilon.
    %
    % Note: You can use predictions = (pval &lt; epsilon) to get a binary vector
    %       of 0's and 1's of the outlier predictions

    cv_pred = pval &lt; epsilon;
    tp = sum((cv_pred == 1) &amp; (yval == 1));
    fp = sum((cv_pred == 1) &amp; (yval == 0));
    fn = sum((cv_pred == 0) &amp; (yval == 1));
    prec = tp / (tp + fp);
    recall = tp / (tp + fn);
    F1 = 2 * prec * recall / (prec + recall);

    % =============================================================

    if F1 &gt; bestF1
       bestF1 = F1;
       bestEpsilon = epsilon;
    end
end

end
</pre>


</div>

</div>

<div id="outline-container-1-12-7" class="outline-4">
<h4 id="sec-1-12-7"><span class="section-number-4">1.12.7</span> collaborative filtering cost function</h4>
<div class="outline-text-4" id="text-1-12-7">

<p><img src="images/collaborative-filtering-cost-function.png"  alt="./images/collaborative-filtering-cost-function.png" />
</p>
<p>
<img src="images/collaborative-filtering-gradient.png"  alt="./images/collaborative-filtering-gradient.png" />
</p>



<pre class="src src-Octave">function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ...
                                  num_features, lambda)
%COFICOSTFUNC Collaborative filtering cost function
%   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...
%   num_features, lambda) returns the cost and gradient for the
%   collaborative filtering problem.
%

% Unfold the U and W matrices from params
X = reshape(params(1:num_movies*num_features), num_movies, num_features);
Theta = reshape(params(num_movies*num_features+1:end), ...
                num_users, num_features);


% You need to return the following values correctly
J = 0;
X_grad = zeros(size(X));
Theta_grad = zeros(size(Theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost function and gradient for collaborative
%               filtering. Concretely, you should first implement the cost
%               function (without regularization) and make sure it is
%               matches our costs. After that, you should implement the
%               gradient and use the checkCostFunction routine to check
%               that the gradient is correct. Finally, you should implement
%               regularization.
%
% Notes: X - num_movies  x num_features matrix of movie features
%        Theta - num_users  x num_features matrix of user features
%        Y - num_movies x num_users matrix of user ratings of movies
%        R - num_movies x num_users matrix, where R(i, j) = 1 if the
%            i-th movie was rated by the j-th user
%
% You should set the following variables correctly:
%
%        X_grad - num_movies x num_features matrix, containing the
%                 partial derivatives w.r.t. to each element of X
%        Theta_grad - num_users x num_features matrix, containing the
%                     partial derivatives w.r.t. to each element of Theta
%

xt = X * Theta'; % m * n * n * u = m * u
df = (xt- Y) .* R; % m * u
J = 0.5 * sum(sum(df .^ 2));
J += 0.5 * lambda * (sum(sum(Theta .^ 2)) + sum(sum(X .^ 2)));


X_grad = df * Theta; % m * u * u * n = m * n;
X_grad += lambda * X;

Theta_grad = df' * X; % u * m * m * n = u * n;
Theta_grad += lambda * Theta;

% =============================================================

grad = [X_grad(:); Theta_grad(:)];

end
</pre>

</div>
</div>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2014-12-04T15:39+0800</p>
<p class="creator"><a href="http://orgmode.org">Org</a> version 7.9.3f with <a href="http://www.gnu.org/software/emacs/">Emacs</a> version 24</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
<!-- Baidu Analytics BEGIN --><script type="text/javascript">var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F54a700ad7035f6e485eaf2300641e7e9' type='text/javascript'%3E%3C/script%3E"));</script><!-- Baidu Analytics END --><!-- Google Analytics BEGIN --><!-- <script type="text/javascript">  var _gaq = _gaq || [];  _gaq.push(['_setAccount', 'UA-31377772-1']);  _gaq.push(['_trackPageview']);  (function() {    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);  })();</script> --><!-- Google Analytics END --><!-- Baidu Button BEGIN --><!-- <script type="text/javascript" id="bdshare_js" data="type=tools&amp;uid=6762177" ></script><script type="text/javascript" id="bdshell_js"></script><script type="text/javascript"> document.getElementById("bdshell_js").src = "http://bdimg.share.baidu.com/static/js/shell_v2.js?cdnversion=" + Math.ceil(new Date()/3600000)</script> --><!-- Baidu Button END --><!-- G+ BEGIN --><!-- Place this render call where appropriate --><!-- <script type="text/javascript">  (function() {    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;    po.src = 'https://apis.google.com/js/plusone.js';    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);  })();</script> --><!-- G+ END --><!-- DISQUS BEGIN --><div id="disqus_thread"></div><script type="text/javascript">/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * *//* required: replace example with your forum shortname  */var disqus_shortname = 'dirlt';var disqus_identifier = 'ml-class.html';var disqus_title = 'ml-class.html';var disqus_url = 'http://dirlt.com/ml-class.html';/* * * DON'T EDIT BELOW THIS LINE * * */(function() {var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><!-- DISQUS END --></body>
</html>
